"""
Reward Calculation Script
"""

import json
import os
import asyncio
from concurrent.futures import ThreadPoolExecutor
from termcolor import cprint
import nest_asyncio
from scipy.stats import norm
from omegaconf import DictConfig, ListConfig, OmegaConf

import math_utils


# ============================================================================
# Configuration Loading
# ============================================================================
def get_config():
    """
    Load configuration from YAML config file and command line arguments
    Command line arguments will override YAML configurations
    """
    cli_conf = OmegaConf.from_cli()
    yaml_conf = OmegaConf.load(cli_conf.config)
    conf = OmegaConf.merge(yaml_conf, cli_conf)
    return conf


# ============================================================================
# Utility Functions
# ============================================================================
def z_score_normalize(lst):
    """
    Z-score normalization
    
    Args:
        lst: List of numerical values
    
    Returns:
        Normalized list
    """
    mean = sum(lst) / len(lst)
    std = (sum((x - mean) ** 2 for x in lst) / len(lst)) ** 0.5
    if std == 0:
        return [0 for x in lst]
    return [(x - mean) / std for x in lst]


def set_last_t(lst: list, t: int) -> None:
    """
    Set the last t elements of the list to max value + 1
    
    Args:
        lst: Input list
        t: Number of tail elements to modify
    
    Returns:
        Modified copy of the list
    """
    new_lst = lst.copy()
    new_val = max(lst) + 1
    new_lst[-t:] = [new_val] * t
    return new_lst


# ============================================================================
# Main Program
# ============================================================================
if __name__ == "__main__":
    
    # ------------------------------------------------------------------------
    # Step 1: Load Configuration
    # ------------------------------------------------------------------------
    config = get_config()
    
    dataset = config.dataset.eval_dataset
    pretrained_model = config.model
    
    # ------------------------------------------------------------------------
    # Step 2: Build Input File Path (using experiment name)
    # ------------------------------------------------------------------------
    result_base_path = config.paths.result_base
    
    # Use experiment name as filename (consistent with eval.py)
    if hasattr(config.experiment, 'name') and config.experiment.name:
        exp_name = config.experiment.name
    else:
        # If no experiment name provided, use default naming convention
        if hasattr(config.experiment, 'model_name') and config.experiment.model_name:
            model_name = config.experiment.model_name
        else:
            model_name = os.path.basename(os.path.normpath(config.model))
        
        block_size = config.rollout.block_size
        dataset = config.dataset.eval_dataset
        exp_name = (f"eval-{model_name}-{dataset}-"
                   f"mt{config.rollout.max_token}-"
                   f"bs{block_size}-"
                   f"ds{config.rollout.denoising_steps_per_block}")
    
    # Handle multi-node case
    num_node = config.experiment.num_node
    node_index = config.experiment.node_index
    
    if num_node > 1:
        input_file = os.path.join(result_base_path, f"outputs-{node_index}-{exp_name}.json")
    else:
        input_file = os.path.join(result_base_path, f"outputs-{exp_name}.json")
    
    cprint(f"Reading result file: {input_file}", "cyan")

    
    # ------------------------------------------------------------------------
    # Step 3: Read results file generated by eval.py
    # ------------------------------------------------------------------------
    try:
        with open(input_file, 'r') as f:
            data = json.load(f)
        cprint(f"Successfully read results for {len(data)} questions", "green")
    except FileNotFoundError:
        cprint(f"Error: File not found {input_file}", "red")
        cprint("Please run eval.py first to generate result file", "yellow")
        exit(1)
    except json.JSONDecodeError as e:
        cprint(f"Error: JSON parsing failed - {e}", "red")
        exit(1)
    
    # ------------------------------------------------------------------------
    # Step 4: Expand data for evaluation
    # ------------------------------------------------------------------------
    index_list = []
    extracted_output_list = []
    ground_truth_list = []
    response_length_list = []
    
    for i in range(len(data)):
        response_length_list.extend(data[i]["response_length"])
        index_list.extend([i] * len(data[i]["extracted_output"]))
        extracted_output_list.extend(data[i]["extracted_output"])
        
        if config.dataset.data_type == "math":
            data[i]["correctness"] = []
            ground_truth_list.extend([data[i]["ground_truth_answer"]] * len(data[i]["extracted_output"]))
    
    cprint(f"Total responses: {len(extracted_output_list)}", "cyan")
    
    # ------------------------------------------------------------------------
    # Step 5: Evaluate Correctness
    # ------------------------------------------------------------------------
    if config.dataset.data_type == "math":
        cprint("Starting math problem evaluation...", "yellow")
        
        nest_asyncio.apply()
        
        async def get_correctness():
            """Concurrently evaluate correctness of all answers"""
            executor = ThreadPoolExecutor(max_workers=64)
            tasks = []
            for i in range(len(index_list)):
                tasks.append(math_utils.is_equal(
                    extracted_output_list[i], 
                    ground_truth_list[i], 
                    executor
                ))
            results = await asyncio.gather(*tasks)
            return results
        
        correctness_list = asyncio.run(get_correctness())
        
        # Add correctness results to data
        for i in range(len(index_list)):
            index_i = index_list[i]
            data[index_i]["correctness"].append(correctness_list[i])
        
        cprint("Math problem evaluation completed", "green")
    
    # ------------------------------------------------------------------------
    # Step 6: Calculate Accuracy
    # ------------------------------------------------------------------------
    if config.dataset.data_type == "math":
        acc = sum(correctness_list) / len(correctness_list)
        correct_count = sum(correctness_list)
        total_count = len(correctness_list)
    else:
        # Code problem evaluation (may have multiple test cases)
        num_task = 0
        num_correct_task = 0
        for x in data:
            for y in x["correctness"]:
                num_correct_task += all(y)
                num_task += 1
        acc = num_correct_task / num_task if num_task else 0
        correct_count = num_correct_task
        total_count = num_task
    
    # ------------------------------------------------------------------------
    # Step 7: Handle step_map (keep or remove based on config)
    # ------------------------------------------------------------------------
    if config.rollout.output_unmasking_history == False:
        for i in range(len(data)):
            data[i]["step_map"] = []
        cprint("Removed step_map to reduce file size", "yellow")
    
    # ------------------------------------------------------------------------
    # Step 8: Save Updated Results (with correctness field added)
    # ------------------------------------------------------------------------
    os.makedirs(os.path.dirname(input_file), exist_ok=True)
    with open(input_file, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    cprint(f"Results updated and saved to: {input_file}", "green")
    
    # ------------------------------------------------------------------------
    # Step 9: Generate Statistics Report
    # ------------------------------------------------------------------------
    # Calculate average response length
    avg_len = sum(response_length_list) / len(response_length_list) if response_length_list else 0
    
    # Calculate pass@k for each question
    per_question_accuracy = []
    for i in range(len(data)):
        if "correctness" in data[i] and data[i]["correctness"]:
            per_question_accuracy.append(any(data[i]["correctness"]))
    
    pass_at_1_or_more = sum(per_question_accuracy) / len(per_question_accuracy) if per_question_accuracy else 0
    
    # Save detailed report (using experiment name)
    result_report_file = os.path.join(result_base_path, f"report-{exp_name}.txt")
    os.makedirs(os.path.dirname(result_report_file), exist_ok=True)
    
    with open(result_report_file, "w", encoding="utf-8") as f:
        def save_and_print(text, color="green"):
            """Save to file and print to terminal simultaneously"""
            cprint(text, color=color)
            f.write(text + "\n")
        
        save_and_print("="*70, "cyan")
        save_and_print("Evaluation Report", "green")
        save_and_print("="*70, "cyan")
        save_and_print(f"\nüè∑Ô∏è  Experiment Name: {exp_name}", "white")
        save_and_print(f"üìä Dataset: {config.dataset.eval_dataset}", "white")
        save_and_print(f"üìù Number of Questions: {len(data)}", "white")
        save_and_print(f"üî¢ Total Responses: {total_count}", "white")
        save_and_print(f"\n‚úÖ Accuracy: {acc:.4f} ({correct_count}/{total_count})", "green")
        save_and_print(f"üéØ Pass@k (at least one correct): {pass_at_1_or_more:.4f} ({sum(per_question_accuracy)}/{len(per_question_accuracy)})", "green")
        save_and_print(f"üìè Average Response Length: {avg_len:.2f} tokens", "yellow")
        save_and_print(f"\nüíæ Detailed Results File: {input_file}", "white")
        save_and_print("="*70 + "\n", "cyan")
    
    cprint(f"Statistics report saved to: {result_report_file}", "green")

    
    # ------------------------------------------------------------------------
    # Step 10: Print Top Error Samples (for analysis)
    # ------------------------------------------------------------------------
    if config.dataset.data_type == "math":
        wrong_samples = []
        for i in range(len(data)):
            if "correctness" in data[i] and not all(data[i]["correctness"]):
                wrong_samples.append({
                    "index": i,
                    "question": data[i]["question"][:100],
                    "ground_truth": data[i]["ground_truth_answer"],
                    "predictions": data[i]["extracted_output"][:3],  # Show first 3 only
                    "correctness": data[i]["correctness"][:3]
                })
        
        if wrong_samples:
            cprint("\nTop 5 Error Samples (for analysis):", "red", attrs=['bold'])
            for idx, sample in enumerate(wrong_samples[:5], 1):
                print(f"\n{idx}. Question[{sample['index']}]: {sample['question']}...")
                print(f"   Ground Truth: {sample['ground_truth']}")
                print(f"   Model Predictions: {sample['predictions']}")
                print(f"   Correctness: {sample['correctness']}")
    
    cprint("\nüéâ Evaluation completed!", "green", attrs=['bold'])